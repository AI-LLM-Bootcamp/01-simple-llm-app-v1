{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d733e1a-fff5-4739-b702-cc4af0a69d41",
   "metadata": {},
   "source": [
    "# How to build a simple LLM App with LangChain and deploy it with LangServe\n",
    "* Very simple LLM App.\n",
    "* Goal of the App: translate text from English into another language.\n",
    "* Second version: how to build this app using a language other than English."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0399355-dece-4701-9bf4-4c204fe74929",
   "metadata": {},
   "source": [
    "## Concepts included\n",
    "* Monitor with LangSmith.\n",
    "* Connect with a LLM.\n",
    "* Use a Prompt Template.\n",
    "* Use an Output Parser.\n",
    "* Chain the Prompt Template, the LLM call and the Output Parser.\n",
    "* Deploy with LangServe."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48386f20-c929-48a9-8720-0953fcd67dd0",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ee060-21f2-4e01-b283-1fd656dac1e9",
   "metadata": {},
   "source": [
    "#### After you download the code from the github repository in your computer\n",
    "In terminal:\n",
    "* cd project_name\n",
    "* pyenv local 3.11.4\n",
    "* poetry install\n",
    "* poetry shell\n",
    "\n",
    "#### To open the notebook with Jupyter Notebooks\n",
    "In terminal:\n",
    "* jupyter lab\n",
    "\n",
    "Go to the folder of notebooks and open the right notebook.\n",
    "\n",
    "#### To see the code in Virtual Studio Code or your editor of choice.\n",
    "* open Virtual Studio Code or your editor of choice.\n",
    "* open the project-folder\n",
    "* open the 001-simpleTranslator.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1fef002-9341-4d66-bd52-209267881002",
   "metadata": {},
   "source": [
    "## Create your .env file\n",
    "* In the github repo we have included a file named .env.example\n",
    "* Rename that file to .env file and here is where you will add your confidential api keys. Remember to include:\n",
    "* OPENAI_API_KEY=your_openai_api_key\n",
    "* LANGCHAIN_TRACING_V2=true\n",
    "* LANGCHAIN_ENDPOINT=https://api.smith.langchain.com\n",
    "* LANGCHAIN_API_KEY=your_langchain_api_key\n",
    "* LANGCHAIN_PROJECT=your_project_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a63ff6-99ff-4629-b965-547d12a99ba6",
   "metadata": {},
   "source": [
    "We will call our LangSmith project **001-simpleTranslator**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06167b66-5893-46ff-b25e-a6adfdd02ced",
   "metadata": {},
   "source": [
    "## Connect with the .env file located in the same directory of this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827b0ea0-ee43-4158-8d5b-9f005c288e98",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7530df5-31ef-40fc-a7ff-4b005169c944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "992ec4a9-aa01-4e44-aeb9-b9a1f3aa9e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.environ[\"OPENAI_API_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c01d18b-f9f0-427b-a9dc-3d1885160578",
   "metadata": {},
   "source": [
    "#### Install LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f154408d-eb7f-46dc-899b-0109fccd9c38",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed746499-d1b8-41e5-b131-270cf5fa229b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc2af3ef-c2c7-445f-92bd-a29c68abce25",
   "metadata": {},
   "source": [
    "## Connect with an LLM and ask it to translate  a sentence from English to Spanish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c689b38-0eee-410a-910f-53c0412e1c0c",
   "metadata": {},
   "source": [
    "If you are using the pre-loaded poetry shell, you do not need to install the following package because it is already pre-loaded for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3fa7337f-3d60-4ede-bdf8-aa7a5cffec43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce3551e-95ca-41a1-8810-89c495bf93ab",
   "metadata": {},
   "source": [
    "#### OpenAI LLM options\n",
    "* See [OpenAI website](https://platform.openai.com/docs/models/gpt-3-5-turbo).\n",
    "* For this project, we will use gpt-3.5-turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9afcbc7d-a816-41e3-925f-850883f5770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a1d409-7f2f-40a4-90c5-ad77dad3edce",
   "metadata": {},
   "source": [
    "* System Message: defines the role played by the LLM.\n",
    "* Human Message: the user input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c5dc0613-fb6d-4c82-a614-9e7307714303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messagesToTheLLM = [\n",
    "    SystemMessage(content=\"Translate the following from English into Spanish\"),\n",
    "    HumanMessage(content=\"Generative AI is the greatest value-creation opportunity in Human History.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479b36ec-341a-47bc-af32-30cfb939810d",
   "metadata": {},
   "source": [
    "#### Step 1: call the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4536f28-32f9-4ef8-9528-b07c37a57388",
   "metadata": {},
   "source": [
    "* `invoke` is the most basic way to call the LLM. We will see more ways of doing it soon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db815e32-8e60-46b3-8cce-fbf40e378397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='La inteligencia artificial generativa es la mayor oportunidad de creación de valor en la historia humana.', response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 32, 'total_tokens': 54}, 'model_name': 'gpt-3.5-turbo', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run-39ae644f-4c3a-4c87-897d-34631695fb86-0')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(messagesToTheLLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9913fc8c-254f-410d-aa8f-35eb0898855e",
   "metadata": {},
   "source": [
    "#### Track the operation in LangSmith\n",
    "* [Open LangSmith here](smith.langchain.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10f836a-a1bd-4458-8972-204d1dfef5dd",
   "metadata": {},
   "source": [
    "## Use an Output Parser to format the response of the LLM as a string of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "54fd7a7d-21ae-4131-bbce-8fba09482bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2df9f80-b57c-4486-b5f6-0c28065a5ddf",
   "metadata": {},
   "source": [
    "#### Step 1: call the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c813944c-a65d-4b82-89b0-bcf034088493",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialResponse = llm.invoke(messagesToTheLLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cc331d-bf66-46fb-a527-dcbaab73d790",
   "metadata": {},
   "source": [
    "#### Step 2: apply the parser to format the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1dd7956e-4e5d-4ca8-b823-c508f2903883",
   "metadata": {},
   "outputs": [],
   "source": [
    "formattedResponse = parser.invoke(initialResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c1077cf9-8e35-411e-9b57-89506c81d4e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La inteligencia artificial generativa es la mayor oportunidad de creación de valor en la historia humana.\n"
     ]
    }
   ],
   "source": [
    "print(formattedResponse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c445f320-289d-4372-93eb-8e32b873b23c",
   "metadata": {},
   "source": [
    "## Chain the LLM Call and the Output Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "07d2d4ed-11f7-41fa-ab1d-22fa4dcac792",
   "metadata": {},
   "outputs": [],
   "source": [
    "llmAndParserChained = llm | parser"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23b3692-a5d7-4550-8717-cb48a8232cd7",
   "metadata": {},
   "source": [
    "#### Steps 1 and 2: call the LLM and apply the parser to the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "da6a7652-268e-42a1-ad80-518a186c07cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "responseFromChain=llmAndParserChained.invoke(messagesToTheLLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab7326c1-4d5a-40e7-9ac0-f04c3baaad7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La inteligencia artificial generativa es la mayor oportunidad de creación de valor en la historia humana.\n"
     ]
    }
   ],
   "source": [
    "print(responseFromChain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b4d263-909c-4bdb-b9f1-e512923c69e8",
   "metadata": {},
   "source": [
    "## Use a Prompt Template with a System Message and Variables\n",
    "* variables:\n",
    "    * text_input: the text the user wants to translate \n",
    "    * language: the language the user wants to translate the text_input into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "61100547-85b6-4bb3-a17d-11a5de63bed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0ed49b3b-c0c1-49cd-b52a-a526563c6b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_template = \"Translate the following text input into {language}:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a12e297-5ead-4c23-b91a-cc1d1c1aaef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_template), \n",
    "        (\"user\", \"{text_input}\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "785aeccb-fed1-4522-8f67-0ed0a05f6b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "SecondChain = prompt_template | llm | parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "12eb314d-e386-4c82-bee8-ed5eb06898fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ResponseFromTheLLM = SecondChain.invoke(\n",
    "    {\n",
    "    \"language\": \"Spanish\",\n",
    "    \"text_input\": \"Now is the moment to learn Generative AI!\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3cc0527f-17b8-4f8c-9b5a-37c5efd18ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¡Ahora es el momento de aprender la IA Generativa!\n"
     ]
    }
   ],
   "source": [
    "print(ResponseFromTheLLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9826d1-5d80-4456-86a6-bfbf69db51c0",
   "metadata": {},
   "source": [
    "## Deploy the App using LangServe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc613d5",
   "metadata": {},
   "source": [
    "**LangServe** is a component of the LangChain framework designed to help developers deploy language model applications as web services easily and efficiently. In simpler terms, it turns your language-based applications (like chatbots, translators, or any other app using language models) into web services that can be accessed over the internet through URLs.\n",
    "\n",
    "Here’s a step-by-step explanation on how to deploy a LangChain app using LangServe:\n",
    "\n",
    "1. **Develop Your App**: First, you create your language model application using LangChain. This involves setting up the language model, defining how it should process inputs and outputs, and possibly integrating with other systems or APIs.\n",
    "\n",
    "2. **Prepare for Deployment**:\n",
    "   - Ensure that your app is well-tested locally and behaves as expected.\n",
    "   - Configure any necessary environment variables, such as API keys or service endpoints, which the app will need to function properly once deployed.\n",
    "\n",
    "3. **Set Up LangServe**:\n",
    "   - LangServe acts as a server environment for your LangChain app. You'll need to configure LangServe settings specific to your application, such as the port number on which the server should run and any specific route paths (URLs) that should be handled by your app.\n",
    "\n",
    "4. **Deploy the Application**:\n",
    "   - Using tools and commands provided by LangChain, you can start the LangServe server with your application loaded on it. This makes your app available on a specified port of your server machine.\n",
    "   - If you need your service to be accessible publicly over the internet (not just locally), you might need to deploy it on a cloud platform or a dedicated server with a public IP address.\n",
    "\n",
    "5. **Access the App via URLs**:\n",
    "   - Once deployed, your application can be accessed through web URLs. This could be for direct user interaction (like a web-based chat interface) or for integration with other systems (like APIs that other software can call to use your app).\n",
    "\n",
    "LangServe effectively simplifies the process of making your LangChain apps accessible as standard web services, which can be a powerful way to deploy AI-driven language applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bcad8f-49ea-4a65-a4f2-531c93c281f0",
   "metadata": {},
   "source": [
    "**You will need to install langserve from terminal**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8c9635ec-22b1-43e2-85f7-316b83380945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install \"langserve[all]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38089b3a-d4cb-49aa-9282-1aaec1a3c391",
   "metadata": {},
   "source": [
    "* See the file 001-simpleTranslator.py in your code editor.\n",
    "* Remember to create .gitignore file and include .env there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "65a0f62a-6ddd-483a-b0d7-801b4bc4c77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from fastapi import FastAPI\n",
    "# from langserve import add_routes\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_core.output_parsers import StrOutputParser\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "# import os\n",
    "\n",
    "# _ = load_dotenv(find_dotenv())\n",
    "\n",
    "# openai_api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "\n",
    "# parser = StrOutputParser()\n",
    "\n",
    "# system_template = \"Translate the following into {language}:\"\n",
    "\n",
    "# prompt_template = ChatPromptTemplate.from_messages([\n",
    "#     ('system', system_template),\n",
    "#     ('user', '{text}')\n",
    "# ])\n",
    "\n",
    "# chain = prompt_template | llm | parser\n",
    "\n",
    "# app = FastAPI(\n",
    "#   title=\"simpleTranslator\",\n",
    "#   version=\"1.0\",\n",
    "#   description=\"A simple API server using LangChain's Runnable interfaces\",\n",
    "# )\n",
    "\n",
    "# add_routes(\n",
    "#     app,\n",
    "#     chain,\n",
    "#     path=\"/chain\",\n",
    "# )\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     import uvicorn\n",
    "\n",
    "#     uvicorn.run(app, host=\"localhost\", port=8000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7645a0cb-c045-4836-a619-63b76fcb09bf",
   "metadata": {},
   "source": [
    "#### Run the server\n",
    "* In terminal:\n",
    "    * python 001-simpleTranslator.py\n",
    "* Check the app in the browser:\n",
    "    * [http://localhost:8000/](http://localhost:8000/)\n",
    "    * Nothing fancy there... yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e003a928-db18-4c16-92e3-594180c4c5ae",
   "metadata": {},
   "source": [
    "#### Now let's try the app using the \"toy demo UI\" provided by LangServe Playground\n",
    "* Try the app in the browser:\n",
    "    * [http://localhost:8000/chain/playground/](http://localhost:8000/chain/playground/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc518dd-a621-4f64-850c-12632133e379",
   "metadata": {},
   "source": [
    "#### And finally, let's see how we can use the LangServe Playground from the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a67a8b87-dadf-448c-a639-168914356140",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'La inteligencia artificial generativa es una oportunidad más grande que Internet.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langserve import RemoteRunnable\n",
    "\n",
    "remote_chain = RemoteRunnable(\"http://localhost:8000/chain/\")\n",
    "remote_chain.invoke({\"language\": \"Spanish\", \"text\": \"Generative AI is a bigger opportunity than Internet\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba16def",
   "metadata": {},
   "source": [
    "## LangServe vs. FastAPI\n",
    "Deploying a LangChain app with LangServe or FastAPI involves similar basic principles—both methods aim to serve your application over the web—but they differ in their level of specialization and the features they offer. Here's a simple explanation of the main differences between these two deployment options:\n",
    "\n",
    "#### FastAPI\n",
    "\n",
    "1. **General-Purpose Framework**: FastAPI is a modern, fast (high-performance) web framework for building APIs with Python. It's designed to be simple to use but powerful in capabilities, supporting the development of robust APIs and web applications.\n",
    "\n",
    "2. **Flexibility**: FastAPI provides extensive flexibility in how you structure your application. It allows for detailed customization of request handling, response formatting, and middleware integration, making it suitable for a wide variety of web services beyond just language applications.\n",
    "\n",
    "3. **Manual Setup**: When deploying a LangChain app with FastAPI, you need to manually set up the routing, request handling, and integration with LangChain. This involves writing more boilerplate code and handling more configuration details.\n",
    "\n",
    "4. **Community and Ecosystem**: FastAPI has a large developer community and a rich ecosystem of plugins and tools, which can be advantageous for solving common web development problems and integrating with other technologies.\n",
    "\n",
    "#### LangServe\n",
    "\n",
    "1. **Specialized for LangChain**: LangServe is tailored specifically for deploying LangChain applications. This specialization means it comes with built-in configurations and setups optimized for language model applications, reducing the need to manually configure many aspects of deployment.\n",
    "\n",
    "2. **Simplicity and Convenience**: LangServe aims to simplify the process of turning your LangChain model into a web service. It abstracts away many of the lower-level details of web service configuration, allowing you to focus more on developing the language model itself.\n",
    "\n",
    "3. **Integrated Tools**: Since LangServe is designed to work seamlessly with LangChain, it often includes tools and features that specifically support language model operations, such as handling different types of language inputs and outputs more effectively.\n",
    "\n",
    "4. **Limited Flexibility**: While offering simplicity, LangServe may not provide as much flexibility as FastAPI in terms of general web development capabilities. It's optimized for a specific type of application, which might limit its utility outside of deploying language models.\n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Choose FastAPI** if you need a more flexible, general-purpose approach that can handle a wide variety of web services, require detailed customization, or want to integrate closely with other web technologies.\n",
    "- **Choose LangServe** if you are focused on deploying LangChain applications quickly and efficiently, prefer simplicity over flexibility, and do not require the extensive capabilities of a full-fledged web framework.\n",
    "\n",
    "Each option has its strengths and is best suited to different types of projects and developer preferences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd262f9-f244-4542-a796-33acaf1656e7",
   "metadata": {},
   "source": [
    "## What if we wanted to build this application in a foreign language, meaning we communicate with the LLM model in a foreign language instead of in English?\n",
    "* See this in 002-simpleTranslatorBuiltInSpanish.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b31534-0694-4352-aa29-1c3371399a69",
   "metadata": {},
   "source": [
    "To modify the LangChain app so that it translates from Spanish to English and communicates with the language model (LLM) in Spanish, you'll need to make several key changes to the prompt template.\n",
    "\n",
    "In this adjusted version:\n",
    "- The `system_template` is set to ask for translations into English but is phrased in Spanish to match the language model's settings.\n",
    "- The FastAPI application details have been updated to reflect the purpose of translating from Spanish to English.\n",
    "\n",
    "This will set up an API that receives text in Spanish, asks the model to translate it into English, and then serves the English translation via an endpoint."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cd908f-a526-410b-bded-566d15c877ba",
   "metadata": {},
   "source": [
    "#### Run the server\n",
    "* In terminal:\n",
    "    * python 002-simpleTranslatorBuiltInSpanish.py\n",
    "* Check the app in the browser:\n",
    "    * [http://localhost:8000/](http://localhost:8000/)\n",
    "    * Nothing fancy there... yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f179b9-34da-48ba-88c3-fcd2a9be06e6",
   "metadata": {},
   "source": [
    "#### Now let's try the app using the \"toy demo UI\" provided by LangServe Playground\n",
    "* Try the app in the browser:\n",
    "    * [http://localhost:8000/chain/playground/](http://localhost:8000/chain/playground/) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d75dd64-cae8-4f99-86e6-8978632d27d8",
   "metadata": {},
   "source": [
    "* Enter \"La inteligencia artificial generativa es la mayor oportunidad de creación de valor en la historia humana.\" and see how it is translated into English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf9c988-530a-430d-83eb-8834e41863f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
